\section{Introduction}

I used the \mintinline{python}{NaiveBayesClassifier} class from the NLTK library. I have tried various preprocessing techniques, and logged the accuracy along with the confusion matrix and metrics such as precision.

\section{Different preprocessing operations}

In this section, I list the accuracies for different preprocessing techniques. I haven't tried all combinations of them, since the number grows exponentially. However, I added each technique \textbf{incrementally} (added the preprocessing step without removing previous ones), with the hope that the change caused by the added technique will be representative of its usefulness.

These trials were made on the \emph{dev} set, since I did them during the development phase. The evaluation in \nameref{sec:evaluation} will be done on the \emph{test} set.

Complete logs from each try is available in the data directory. These include the confusion matrix, accuracy, and for each category the precision, recall and $F_1\text{ measure}$. I include only the last log here, to keep this report short.

Details of the calculation of precision, recall and $F_1\text{ measure}$ are given in \nameref{sec:calculation_metrics}.

A table of the accuracies for each technique is in \autoref{tab:bayes_table}. As seen in the table, each techniques improved the accuracy in SVC. But for Naive Bayes, removing the punctuation and stemming caused a decrease in accuracy.

% table of accuracies
\begin{table}[ht]
    \centering
    \caption{Accuracies for different preprocessing techniques.}
    \label{tab:bayes_table}
    \vspace{1em}
    \begin{tabular}[htpb]{c|c|c}
        \multirow{2}{*}{Preprocessing type} & \multicolumn{2}{c}{Accuracy (\%)} \\
        \cline{2-3}
                                            &
        Naive Bayes
                                            & Support Vector                    \\
        \hline\hline
        \nameref{sec:simple}
                                            & $64.20$
                                            & $64.09$                           \\
        \hline
        \nameref{sec:lowercase}
                                            & $65.92$
                                            & $66.77$                           \\
        \hline
        \nameref{sec:punctuation}
                                            & $65.38$
                                            & $68.70$                           \\
        \hline
        \nameref{sec:stopwords}
                                            & $67.63$
                                            & $70.74$                           \\
        \hline
        \nameref{sec:stemming}              & $66.67$
                                            & $72.34$                           \\
        \hline
        \nameref{sec:shortwords}            & $66.99$
                                            & $72.78$                           \\
    \end{tabular}
\end{table}

\subsection{Simplest version}
\label{sec:simple}

The words in title are counted twice --- in this version and in all the following ones. Other than this, there is no processing in this version. The text is given to \mintinline{python}{nltk.word_tokenize()} and the resulting tokens are used to train the classifier.


\subsection{Lowercase conversion}
\label{sec:lowercase}

Converted all words to lowercase.

This improved accuracy in both SVC and NBC. I think this is because our corpus size is rather small. So, this change allows us to make better use of the limited data.

I think, on a larger corpus, keeping the case could be more beneficial, since it would allow us to distinguish the words in the title from the words in the body.

\subsection{Punctuation removal}
\label{sec:punctuation}

Removed the following characters from the text:
\verbatiminput{punctuation.txt}

I did not replace them with spaces, but simply removed them. This made, for example, the text \emph{Sophies's} to become the single word \emph{sophies}.

Removing the punctuation caused a decrease in the accuracy of NB classifier. I was expecting a decrease, since punctuation can actually be helpful in understanding the book's genre. For example, one could expect to see more question marks in a mystery book's description. However, you wouldn't expect lots of exclamation marks in a science book, unless the author got very excited about whatever scientific topic they were writing about.

SVC's accuracy increased.

\subsection{Stopword removal}
\label{sec:stopwords}

I used the corpus \mintinline{python}{nltk.corpus.stopwords.words('english')} from the NLTK library. I removed every word that occured in this list.

This improved the accuracy of both classifiers.

\subsection{Stemming}
\label{sec:stemming}

I passed each word to the \mintinline{python}{nltk.stem.PorterStemmer()} from the NLTK library.

This decreased the accuracy of NB, and resulted in a significant improvement in SVC.

\subsection{Removing short words}
\label{sec:shortwords}

I thought stopword removal should have been enough, but I also tried removing words that were shorter than 3 characters. Surprizingly, this resulted in an increase in the accuracy of both classifiers.

\section{Evaluation}
\label{sec:evaluation}

\subsection{Calculation of metrics}
\label{sec:calculation_metrics}

Recall is the ratio of true positives for a class to the number of input documents of that type. To find recall, we divide each diagonal entry by the sum of corresponding row.

Precision is the ratio of true positives for a class to the number of documents that are identified to be in that class. To calculate it, we divide diagonal entries by the sum in that column.

\subsection{Metrics for SVC and NBC}

The confusion matrix and accuracy of the classifiers; along with the precision, recall and $F_1\text{ measure}$ for each category are given in \autoref{fig:svc_best_metrics} and \autoref{fig:bayes_best_metrics}.

Confusion matrices here were created by the \mintinline{python}{nltk.classify.util.ConfusionMatrix} class from the NLTK library.

Per-category metrics were calculated as follows:
\inputminted[]{python}{metrics.py}

\begin{figure}[htpb]
    \caption{Metrics for the best SVC version on test data set}
    \label{fig:svc_best_metrics}
    \begin{tcolorbox}
        \verbatiminput{data/classifier_svc_lowercase_removepunc_stopword_stem_3ch_test.log}
    \end{tcolorbox}
\end{figure}

\begin{figure}[htpb]
    \caption{Metrics for the best NB version on test data set}
    \label{fig:bayes_best_metrics}
    \begin{tcolorbox}
        \verbatiminput{data/classifier_bayes_best_test.log}
    \end{tcolorbox}
\end{figure}
