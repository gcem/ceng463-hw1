\section{Introduction}

In this assignment, I used the \mintinline{python}{NaiveBayesClassifier} class from the NLTK library for Naive Bayes Classifier, and \mintinline{python}{SklearnClassifier} class from NLTK along with \mintinline{python}{SVC} class from scikit-learn for the Support Vector Classifier.

I have tried various preprocessing operations, and logged the accuracy at each step, along with the confusion matrix and metrics such as precision.

For the most of the assignment, I used the count of single words as features. In the end, I tried changing the features to bigrams. I list its results at the end of this report.

\section{Different preprocessing operations}
\label{sec:preprocessing}

In this section, I list the accuracies for different preprocessing techniques. I haven't tried all combinations of them, since the number grows exponentially. However, I added each technique \textbf{incrementally} (added the preprocessing step without removing previous ones), with the hope that the change caused by the added technique will be representative of its usefulness.

These trials were made on the \emph{dev} set, since I did them during the development phase. The evaluation in \nameref{sec:evaluation} will be done on the \emph{test} set.

Complete logs from each try is available in the data directory. These include the confusion matrix, accuracy, and for each category the precision, recall and $F_1\text{ measure}$. I include only the last log here, to keep this report short.

Details of the calculation of precision, recall and $F_1\text{ measure}$ are given in \nameref{sec:calculation_metrics}.

A table of the accuracies for each technique is in \autoref{tab:bayes_table}. I recorded these values during development, so these are for the \emph{dev} set. As seen in the table, each techniques improved the accuracy in SVC. But for Naive Bayes, removing the punctuation and stemming caused a decrease in accuracy.

% table of accuracies
\begin{table}[ht]
    \centering
    \caption{Accuracies for different preprocessing techniques.}
    \label{tab:bayes_table}
    \vspace{1em}
    \begin{tabular}[htpb]{c|c|c}
        \multirow{2}{*}{Preprocessing type} & \multicolumn{2}{c}{Accuracy (\%)} \\
        \cline{2-3}
                                            &
        Naive Bayes
                                            & Support Vector                    \\
        \hline\hline
        \nameref{sec:simple}
                                            & $64.20$
                                            & $64.09$                           \\
        \hline
        \nameref{sec:lowercase}
                                            & $65.92$
                                            & $66.77$                           \\
        \hline
        \nameref{sec:punctuation}
                                            & $65.38$
                                            & $68.70$                           \\
        \hline
        \nameref{sec:stopwords}
                                            & $67.63$
                                            & $70.74$                           \\
        \hline
        \nameref{sec:stemming}              & $66.67$
                                            & $72.34$                           \\
        \hline
        \nameref{sec:shortwords}            & $66.99$
                                            & $72.78$                           \\
    \end{tabular}
\end{table}

\subsection{Simplest version}
\label{sec:simple}

The words in title are counted twice --- in this version and in all the following ones. Other than this, there is no processing in this version. The text is given to \mintinline{python}{nltk.word_tokenize()} and the resulting tokens are used to train the classifier.


\subsection{Lowercase conversion}
\label{sec:lowercase}

Converted all words to lowercase.

This improved accuracy in both SVC and NBC. I think this is because our corpus size is rather small. So, this change allows us to make better use of the limited data.

I think, on a larger corpus, keeping the case could be more beneficial, since it would allow us to distinguish the words in the title from the words in the body.

\subsection{Punctuation removal}
\label{sec:punctuation}

Removed the following characters from the text:
\verbatiminput{punctuation.txt}

I did not replace them with spaces, but simply removed them. This made, for example, the text \emph{Sophies's} to become the single word \emph{sophies}.

Removing the punctuation caused a decrease in the accuracy of NB classifier. I was expecting a decrease, since punctuation can actually be helpful in understanding the book's genre. For example, one could expect to see more question marks in a mystery book's description. However, you wouldn't expect lots of exclamation marks in a science book, unless the author got very excited about whatever scientific topic they were writing about.

SVC's accuracy increased.

\subsection{Stopword removal}
\label{sec:stopwords}

I used the corpus \mintinline{python}{nltk.corpus.stopwords.words('english')} from the NLTK library. I removed every word that occured in this list.

This improved the accuracy of both classifiers.

\subsection{Stemming}
\label{sec:stemming}

I passed each word to the \mintinline{python}{nltk.stem.PorterStemmer()} from the NLTK library.

This decreased the accuracy of NB, and resulted in a significant improvement in SVC.

\subsection{Removing short words}
\label{sec:shortwords}

I thought stopword removal should have been enough, but I also tried removing words that were shorter than 3 characters. Surprizingly, this resulted in an increase in the accuracy of both classifiers.

\section{Evaluation}
\label{sec:evaluation}

I evaluated the classifiers with the best preprocessing operations based on the \emph{dev} set. For SVC, I kept all the operations listed in \nameref{sec:preprocessing}. For NB, I deleted the stemming and punctuation removal code.

\subsection{Calculation of metrics}
\label{sec:calculation_metrics}

Recall is the ratio of true positives for a class to the number of input documents of that type. To find recall, we divide each diagonal entry by the sum of corresponding row.

Precision is the ratio of true positives for a class to the number of documents that are identified to be in that class. To calculate it, we divide diagonal entries by the sum in that column.

$F_1\text{ measure}$ is the harmonic mean of precision and recall.

These three per-category metrics listed in \nameref{sec:metrics_svc_nbc} were calculated by the following code:
\inputminted[]{python}{metrics.py}

Confusion matrices were created by the \mintinline{python}{nltk.classify.util.ConfusionMatrix} class from the NLTK library.

\subsection{Metrics for SVC and NBC}
\label{sec:metrics_svc_nbc}

The confusion matrix and accuracy of the classifiers; along with the precision, recall and $F_1\text{ measure}$ for each category are given in \autoref{fig:bayes_best_metrics} and \autoref{fig:svc_best_metrics}.


\begin{figure}[htpb]
    \caption{Confusion matrix and other metrics for the best NB version on test data set}
    \label{fig:bayes_best_metrics}
    \begin{tcolorbox}[title=NB - Bag of words]
        \verbatiminput{data/classifier_bayes_best_test.log}
    \end{tcolorbox}
\end{figure}

\begin{figure}[htpb]
    \caption{Confusion matrix and other metrics for the best SVC version on test data set}
    \label{fig:svc_best_metrics}
    \begin{tcolorbox}[title=SVC - Bag of words]
        \verbatiminput{data/classifier_svc_lowercase_removepunc_stopword_stem_3ch_test.log}
    \end{tcolorbox}
\end{figure}

\section{Comparison}

NB and SVC have a similar accuracy with this choice of features. However, they respond to changes in the preprocessing step differently.

\section{Discussion of confusion matrices}

As the matrices in \autoref{fig:bayes_best_metrics} and \autoref{fig:svc_best_metrics} show, some genres are more difficult to distinguish.

For example, horror and science-fiction books are not distinguished by NB very precisely. Similarly, many mystery books were classified by the NBC as horror books. But not as many horror books were not classified as mystery books. This shows that the classifier tends to choose horror more often, when the description is not very helpful.

There are also genres that are easily distinguished. For example, no mystery or horror books were classified wrongly as philosophy books by NBC. Similarly, three are very few false positives for the sports category.

\section{Bigrams as features}

I followed a "bag of words" approach in the tests mentioned above. I used the single words (or punctuation characters) as features, along with their counts.

In this section, I will show the results I got, when I used word pairs as features. This is similar to the 2-gram language models we discussed.

I implemented this change on the final versions of the code discussed above. That is, I combined the adjacent words after removing stopwords.

In Naive Bayes Classifier, this gave a decent result, but it wasn't as good as the results from the bag of words approach. In the SVC, results were horrible. About all documents were being classified as "horror", which was the first in the list of labels. I tried SVC without stemming, but it didn't help.

Results of this modification are shown in \autoref{fig:bayes_wordpair_test} and \autoref{fig:svc_nbc_wordpair_test} with red background.

\

\begin{figure}[htpb]
    \caption{NB with word pairs as features}
    \label{fig:bayes_wordpair_test}
    \begin{tcolorbox}[colback=red!30!white,
            title=NB - Word pairs]
        \verbatiminput{data/feature_bayes_best_test.log}
    \end{tcolorbox}
\end{figure}

\begin{figure}[htpb]
    \caption{SVC with word pairs as features}
    \label{fig:svc_nbc_wordpair_test}
    \begin{tcolorbox}[colback=red!30!white,
            title=SVC - Word pairs]
        \verbatiminput{data/feature_svc_lowercase_removepunc_stopword_stem_3ch_test.log}
    \end{tcolorbox}
\end{figure}