\section{Naive Bayes Classifier}
\label{sec:bayes}

\subsection{Introduction}

I used the \mintinline{python}{NaiveBayesClassifier} class from the NLTK library. I have tried various preprocessing techniques, and logged the accuracy along with the confusion matrix and metrics such as precision.

\subsection{Metrics for different preprocessing techniques}

In this section, I list the accuracies for different preprocessing techniques. I haven't tried all combinations of them, since the number grows exponentially. However, I added each technique \textbf{incrementally} (added the preprocessing step without removing previous ones), with the hope that the change caused by the added technique will be representative of its usefulness.

These trials were made on the \emph{dev} set, since I did them during the development phase. The evaluation in \autoref{sec:bayes_evaluation} will be done on the \emph{test} set.

Complete logs from each try is available in the data directory. These include the confusion matrix, accuracy, and for each category the precision, recall and $F_1\text{ measure}$. I include only the last log here, to keep this report short.

Details of the calculation of precision, recall and $F_1\text{ measure}$ are given in \autoref{sec:bayes_evaluation}.

A table of the accuracies for each technique is in \autoref{tab:bayes_table}. As seen in the table, each techniques improved the accuracy in SVC. But for Naive Bayes, removing the punctuation and stemming caused a decrease in accuracy.

% table of accuracies
\begin{table}[ht]
    \centering
    \caption{Accuracies for different preprocessing techniques.}
    \label{tab:bayes_table}
    \vspace{1em}
    \begin{tabular}[htpb]{c|c|c}
        \multirow{2}{*}{Preprocessing type} & \multicolumn{2}{c}{Accuracy (\%)} \\
        \cline{2-3}
                                            &
        Naive Bayes
                                            & Support Vector                    \\
        \hline\hline
        \nameref{sec:bayes_simple}
                                            & $64.20$
                                            & $64.09$                           \\
        \hline
        \nameref{sec:bayes_lowercase}
                                            & $65.92$
                                            & $66.77$                           \\
        \hline
        \nameref{sec:bayes_punctuation}
                                            & $65.38$
                                            & $68.70$                           \\
        \hline
        \nameref{sec:bayes_stopwords}
                                            & $67.63$
                                            & $70.74$                           \\
        \hline
        \nameref{sec:bayes_stemming}        & $66.67$
                                            & $72.34$                           \\
        \hline
        \nameref{sec:bayes_shortwords}      & $66.99$
                                            & $72.78$                           \\
    \end{tabular}
\end{table}

\subsubsection{Simplest version}
\label{sec:bayes_simple}

The words in title are counted twice --- in this version and in all the following ones. Other than this, there is no processing in this version. The text is given to \mintinline{python}{nltk.word_tokenize()} and the resulting tokens are used to train the classifier.

Accuracy in this version is $64.2\%$. Confusion matrix and other metrics for each category are listed in \autoref{fig:bayes_simplest_metrics}.

% include data/bayes_simple.txt in a textbox
\begin{figure}[htpb]
    \caption{Metrics for the simplest version}
    \label{fig:bayes_simplest_metrics}
    \begin{tcolorbox}
        \verbatiminput{data/classifier_bayes_simple.log}
    \end{tcolorbox}
\end{figure}

\subsubsection{Lowercase conversion}
\label{sec:bayes_lowercase}

Converted all words to lowercase.

\subsubsection{Punctuation removal}
\label{sec:bayes_punctuation}

Removed the following characters from the text:
\verbatiminput{punctuation.txt}

I did not replace them with spaces, but simply removed them. This made, for example, the text \emph{Sophies's} to become the single word \emph{sophies}.

\subsubsection{Stopword removal}
\label{sec:bayes_stopwords}

I used the corpus \mintinline{python}{nltk.corpus.stopwords.words('english')} from the NLTK library. I removed every word that occured in this list.

\subsubsection{Stemming}
\label{sec:bayes_stemming}

I passed each word to the \mintinline{python}{nltk.stem.PorterStemmer()} from the NLTK library.

\subsubsection{Removing short words}
\label{sec:bayes_shortwords}

I thought stopword removal should have been enough, but I also tried removing words that were shorter than 3 characters. This resulted in a small increase in accuracy.

\subsection{Evaluation}
\label{sec:bayes_evaluation}

\subsubsection{Calculation of metrics}
Recall is the ratio of true positives for a class to the number of input documents of that type. To find recall, we divide each diagonal entry by the sum of corresponding row.

Precision is the ratio of true positives for a class to the number of documents that are identified to be in that class. To calculate it, we divide diagonal entries by the sum in that column.
